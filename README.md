<br>

<p align="center">
  <a href="https://havenllm.com"><img src="https://raw.githubusercontent.com/havenhq/haven/dev/logo.png" width="300"/></a>
</p>

<p align="center">
    <b>Fine-Tune and Deploy LLMs On Your Own Infrastructure</b>
</p>

<div align="center">

[ğŸ’» Quickstart](https://docs-havenhq.vercel.app/)
<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
[ğŸ  Website]()
<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
[ğŸ“„ Docs](https://docs-havenhq.vercel.app/)
<span>&nbsp;&nbsp;â€¢&nbsp;&nbsp;</span>
[ğŸ’¬ Slack]()
<br>
<p align="center">
    Haven lets you build LLM-powered applications <b>hosted entirely on your own infrastructure</b>.<br>
    Just select a model to run - Haven will set up a production-ready 
  API server in your private cloud.
</p>


</div>


<br>
<br>



## Getting Started ğŸ”¥

Setting up an LLM server requires just three steps:

1. Get an API key for a Google Cloud service account
2. Deploy Haven's manager container on a Google Cloud instance
3. Spin up a model worker using the Python SDK

A description of these steps can be found in our [documentation](https://docs-havenhq.vercel.app/). 

<br>

## Roadmap ğŸš€

We're constantly building new features and would love your feedback! Here's what we are currently looking to integrate into our platform:

- [x] Inference Workers
- [x] Google Cloud Support
- [ ] Fine-Tuning Workers
- [ ] AWS Support

<br>

## Learn More ğŸ”

To learn more about our platform, you should refer to our [documentation](https://docs-havenhq.vercel.app/).
